{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "cxWzj2YUrb_-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Lambda, Dense, Flatten, Conv2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ahMvJ9wwhw1j",
    "outputId": "f5a9843f-cbf9-440c-a494-2956b70ba1bf"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1iCM30darcAC",
    "outputId": "5622ce8a-26d5-475b-eae7-9df4c99cf739"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lQikOCT9rcAE"
   },
   "outputs": [],
   "source": [
    "# import splitfolders\n",
    "# input_folder = \"Garbage_Collective_Data/\"\n",
    "# splitfolders.ratio(input_folder, output = \"garbage_class\",\n",
    "#                    seed = 0,ratio = (0.70,0.20,0.10),\n",
    "#                    group_prefix = None )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jAlsP2m9rcAF"
   },
   "outputs": [],
   "source": [
    "img_size = [400,400]\n",
    "\n",
    "train_path = 'garbage_class/train'\n",
    "valid_path = 'garbage_class/val'\n",
    "test_path = 'garbage_class/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "5YI8ofbH-JlK"
   },
   "outputs": [],
   "source": [
    "# y_pred = model.predict(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "wNOCFmQkrcAG"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PE-bddscrcAH",
    "outputId": "61c113d1-d053-4bcc-a832-5701d5528b9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 224, 224, 16)      208       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 112, 112, 16)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 112, 112, 32)      2080      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 56, 56, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 56, 56, 64)        8256      \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 28, 28, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 28, 28, 128)       32896     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 14, 14, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 14, 14, 256)       131328    \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 7, 7, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 12544)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 500)               6272500   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 12)                6012      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,453,280\n",
      "Trainable params: 6,453,280\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "model.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(224,224,3)))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation =\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=64,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=128,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=256,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500,activation=\"relu\"))\n",
    "model.add(Dense(12,activation=\"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "3OENBBuj5Zp8"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  loss='categorical_crossentropy',\n",
    "  optimizer='adam',\n",
    "  metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "BgldIom15ZmZ"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UjaGn0ew5Zj6",
    "outputId": "20ededfe-d603-4ba3-9e6e-4b1f82218e1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10911 images belonging to 12 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = train_datagen.flow_from_directory(train_path,\n",
    "                                                 target_size = (224, 224),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ctJjlhtc5Zhv",
    "outputId": "4fe1a27c-ad0b-4494-b7b9-28be3d898214"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.preprocessing.image.DirectoryIterator at 0x158518b59d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KU1yoPGA5Ze8",
    "outputId": "b250fd30-0bc3-4581-e2bb-ff192ed81dfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3116 images belonging to 12 classes.\n"
     ]
    }
   ],
   "source": [
    "val_set = test_datagen.flow_from_directory(valid_path,\n",
    "                                            target_size = (224, 224),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bYeoxzOt5ZVo",
    "outputId": "364fbec7-d695-49c7-d117-f3388c986225"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.preprocessing.image.DirectoryIterator at 0x1585dbcd850>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mb4-RIgf77Qi",
    "outputId": "b9f77bcb-9a90-44ed-ae6e-8de56ff0d21c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1569 images belonging to 12 classes.\n"
     ]
    }
   ],
   "source": [
    "test_set = test_datagen.flow_from_directory(test_path,\n",
    "                                            target_size = (224, 224),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DNO9Lyah77TX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aQrSK9Vy5keD",
    "outputId": "d80f6c3f-d942-4676-d868-109fed53b99b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'battery': 0, 'biological': 1, 'brown_glass': 2, 'cardboard': 3, 'clothes': 4, 'green_glass': 5, 'metal': 6, 'paper': 7, 'plastic': 8, 'shoes': 9, 'trash': 10, 'white_glass': 11}\n"
     ]
    }
   ],
   "source": [
    "print(val_set.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "houxJlac5mrI",
    "outputId": "1276b1c3-6f04-4c4e-8cd0-d5a503c83f8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "341/341 [==============================] - 397s 1s/step - loss: 0.4673 - accuracy: 0.8436 - val_loss: 0.7349 - val_accuracy: 0.7770\n"
     ]
    }
   ],
   "source": [
    "cnn_model = model.fit(\n",
    "  training_set,\n",
    "  validation_data=val_set,\n",
    "  epochs=1,\n",
    "  steps_per_epoch=len(training_set),\n",
    "  validation_steps=len(val_set)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QhZJt_8w7lHK",
    "outputId": "4827c170-f42b-45c2-d664-9bc35a9db8de"
   },
   "outputs": [],
   "source": [
    "model.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 533
    },
    "id": "rZBITMcD7i7d",
    "outputId": "cc25219a-a0e7-4217-b0b2-7e0d834c58cc"
   },
   "outputs": [],
   "source": [
    "# plot the loss\n",
    "plt.plot(cnn_model.history['loss'], label='train loss')\n",
    "plt.plot(cnn_model.history['val_loss'], label='val loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('LossVal_loss')\n",
    "\n",
    "\n",
    "# plot the accuracy\n",
    "plt.plot(cnn_model.history['accuracy'], label='train acc')\n",
    "plt.plot(cnn_model.history['val_accuracy'], label='val acc')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('AccVal_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "FAr7Redy6_YX"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Do3ElFeU8D8i",
    "outputId": "1776794a-59b9-4a0a-b311-bfe0a3f1221a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.54056537e-01, 1.91570329e-03, 1.22033758e-04, ...,\n",
       "        2.62807636e-03, 1.83656007e-01, 3.40135276e-01],\n",
       "       [1.55460224e-01, 1.48912128e-02, 2.73597315e-02, ...,\n",
       "        2.27141147e-03, 9.24384745e-04, 2.68405471e-02],\n",
       "       [4.21448112e-06, 1.37220384e-04, 2.10995612e-08, ...,\n",
       "        5.35007217e-04, 1.36351655e-05, 2.75464595e-06],\n",
       "       ...,\n",
       "       [1.57955840e-01, 1.25251466e-03, 1.26426705e-04, ...,\n",
       "        7.19557166e-01, 1.13316655e-05, 1.18018892e-04],\n",
       "       [1.14498485e-03, 4.59776324e-08, 2.14731913e-10, ...,\n",
       "        5.42112030e-05, 1.09794472e-10, 2.16687980e-07],\n",
       "       [1.01066571e-05, 2.53082305e-07, 1.62145386e-06, ...,\n",
       "        5.66623748e-05, 1.13823035e-06, 4.07654268e-04]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dDc26LPK8HDE"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "pred = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "za4vIITU9RbF",
    "outputId": "0c8bec62-a579-4ad7-cc4e-1e176c58cfb8"
   },
   "outputs": [],
   "source": [
    "# for i,j in zip(list(pred),list(test_set.classes)):\n",
    "#   print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TW91mJPO9SZL"
   },
   "outputs": [],
   "source": [
    "# print(list(test_set.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g7G0rn6j_Kp9"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model.save('model_cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5u1VFOgF_3PL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5N6jLo3p_3Ma"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "tDvCvZwC_3Jz"
   },
   "outputs": [],
   "source": [
    "img=image.load_img(\"C:/Users/Anubhav/Downloads/istockphoto-538807106-1024x1024.jpg\",target_size=(224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pXUrZxe7_3G8",
    "outputId": "095349f6-3a22-46d2-ca84-088f4a3e9ac7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[178., 167., 111.],\n",
       "        [187., 168., 138.],\n",
       "        [255., 255., 239.],\n",
       "        ...,\n",
       "        [132., 139., 145.],\n",
       "        [185., 195., 204.],\n",
       "        [233., 241., 244.]],\n",
       "\n",
       "       [[163., 153., 100.],\n",
       "        [150., 133., 105.],\n",
       "        [255., 255., 240.],\n",
       "        ...,\n",
       "        [194., 192., 193.],\n",
       "        [124., 136., 150.],\n",
       "        [142., 158., 171.]],\n",
       "\n",
       "       [[145., 138.,  92.],\n",
       "        [164., 148., 122.],\n",
       "        [252., 254., 240.],\n",
       "        ...,\n",
       "        [203., 206., 213.],\n",
       "        [193., 201., 214.],\n",
       "        [147., 156., 165.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[219., 223., 224.],\n",
       "        [221., 225., 226.],\n",
       "        [200., 204., 205.],\n",
       "        ...,\n",
       "        [181., 183., 180.],\n",
       "        [169., 171., 168.],\n",
       "        [154., 156., 153.]],\n",
       "\n",
       "       [[206., 210., 211.],\n",
       "        [211., 215., 216.],\n",
       "        [199., 203., 204.],\n",
       "        ...,\n",
       "        [175., 177., 176.],\n",
       "        [156., 158., 157.],\n",
       "        [163., 165., 164.]],\n",
       "\n",
       "       [[197., 198., 200.],\n",
       "        [203., 207., 208.],\n",
       "        [185., 191., 191.],\n",
       "        ...,\n",
       "        [177., 179., 178.],\n",
       "        [157., 159., 158.],\n",
       "        [238., 240., 239.]]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=image.img_to_array(img)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z-np9Wt3AcHP",
    "outputId": "251bd3bc-bd2b-47aa-923c-285402207b21"
   },
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "2xctmpBFAcCo"
   },
   "outputs": [],
   "source": [
    "x=x/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "IVkwHFOJAv1S"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tov6HyIJAb_4",
    "outputId": "d87c77a0-863e-4c0c-b145-a2f167129fa7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 224, 224, 3)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=np.expand_dims(x,axis=0)\n",
    "# # img_data=preprocess_input(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kQyu3dO7_27A",
    "outputId": "44ee7e63-4608-45db-e4ea-a10a79ffcef7"
   },
   "outputs": [],
   "source": [
    "preds = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.imagenet_utils import preprocess_input, decode_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`decode_predictions` expects a batch of predictions (i.e. a 2D array of shape (samples, 1000)). Found array with shape: (1, 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [43]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#pred_class = preds.argmax(axis=-1)            # Simple argmax\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m pred_class \u001b[38;5;241m=\u001b[39m \u001b[43mdecode_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\applications\\imagenet_utils.py:147\u001b[0m, in \u001b[0;36mdecode_predictions\u001b[1;34m(preds, top)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m CLASS_INDEX\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(preds\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m preds\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1000\u001b[39m:\n\u001b[1;32m--> 147\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`decode_predictions` expects \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    148\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma batch of predictions \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    149\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(i.e. a 2D array of shape (samples, 1000)). \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    150\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFound array with shape: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(preds\u001b[38;5;241m.\u001b[39mshape))\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m CLASS_INDEX \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    152\u001b[0m   fpath \u001b[38;5;241m=\u001b[39m data_utils\u001b[38;5;241m.\u001b[39mget_file(\n\u001b[0;32m    153\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimagenet_class_index.json\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    154\u001b[0m       CLASS_INDEX_PATH,\n\u001b[0;32m    155\u001b[0m       cache_subdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    156\u001b[0m       file_hash\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc2c37ea517e94d9795004a39431a14cb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: `decode_predictions` expects a batch of predictions (i.e. a 2D array of shape (samples, 1000)). Found array with shape: (1, 12)"
     ]
    }
   ],
   "source": [
    "#pred_class = preds.argmax(axis=-1)            # Simple argmax\n",
    "pred_class = decode_predictions(preds, top=1)   # ImageNet Decode\n",
    "# result = str(pred_class[0][0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "rnTVH0kxA1oG"
   },
   "outputs": [],
   "source": [
    "\n",
    "a = np.argmax(model.predict(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_BPoU8vqBWn_",
    "outputId": "5ce9ac95-3fe9-4179-db91-d42032d9e7b4"
   },
   "outputs": [],
   "source": [
    "if(a==0):\n",
    "  print(\"battery\")\n",
    "elif(a==1):\n",
    "  print(\"biological\")\n",
    "elif(a==2):\n",
    "  print(\"brown_glass\")\n",
    "elif(a==3):\n",
    "  print(\"cardboard\")\n",
    "elif(a==4):\n",
    "  print(\"clothes\")\n",
    "elif(a==5):\n",
    "  print(\"green_glass\")\n",
    "elif(a==6):\n",
    "  print(\"metal\")\n",
    "elif(a==7):\n",
    "  print(\"paper\")\n",
    "elif(a==8):\n",
    "  print(\"plastic\")\n",
    "elif(a==9):\n",
    "  print(\"shoes\")\n",
    "elif(a==10):\n",
    "  print(\"trash\")\n",
    "elif(a==11):\n",
    "  print(\"white_glass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eLix5XZrGunn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "name": "Garbage Classification CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
